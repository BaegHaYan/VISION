{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fd8ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "from PIL import Image, ImageFile\n",
    "from glob import glob\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab80a6",
   "metadata": {},
   "source": [
    "'''\n",
    "{'filename': '5f656a0f627a3ef96dec882437e3e7ada1c7a877201cf54dcd7a2c4508588ff3_여_30_기쁨_공공시설&종교&의료시설_20201204105732-001-007.jpg',\n",
    " 'gender': '여',\n",
    " 'age': 30,\n",
    " 'isProf': '전문인',\n",
    " 'faceExp_uploader': '기쁨',\n",
    " 'bg_uploader': '공공시설/종교/의료시설',\n",
    " 'annot_A': {'boxes': {'maxX': 1912.2253,\n",
    "   'maxY': 1581.6027,\n",
    "   'minX': 1187.4949,\n",
    "   'minY': 579.22235},\n",
    "  'faceExp': '기쁨',\n",
    "  'bg': '공공시설/종교/의료'},\n",
    " 'annot_B': {'boxes': {'maxX': 1912.348108621648,\n",
    "   'maxY': 1572.1522585800617,\n",
    "   'minX': 1206.363701502596,\n",
    "   'minY': 579.1777983055337},\n",
    "  'faceExp': '기쁨',\n",
    "  'bg': '공공시설/종교/의료'},\n",
    " 'annot_C': {'boxes': {'maxX': 1890.909447114109,\n",
    "   'maxY': 1567.448627450284,\n",
    "   'minX': 1183.8414475546967,\n",
    "   'minY': 596.9434661684523},\n",
    "  'faceExp': '기쁨',\n",
    "  'bg': '공공시설/종교/의료'}}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf704aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaegDataset(Dataset):\n",
    "    def __init__(self , transform = None):\n",
    "        self.transform = transform\n",
    "        # image dataset\n",
    "        # image dataset 병합\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        self.data_list = glob('/data/Emotion_data/Validation/image/*')\n",
    "        self.label_list = glob(\"/data/Emotion_data/Validation/label/*\")\n",
    "            \n",
    "        # label map\n",
    "        self.label_map = {\n",
    "            '기쁨' : 0,\n",
    "            '상처' : 1,\n",
    "            '당황' : 2,\n",
    "            '분노' : 3,\n",
    "            '불안' : 4,\n",
    "            '슬픔' : 5,\n",
    "            '중립' : 6\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # load images and mask\n",
    "        img_path = self.data_list[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform  is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        # 1. filename만 따로 빼서 for문 돌려서 json_list에 있는 것과 비교\n",
    "        img_name = img_path.split('/')\n",
    "        mask = {}\n",
    "        for json_list in self.label_list:\n",
    "            with open(json_list, 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "                for i in range(0, len(json_data)):\n",
    "                    filename = json_data[i]['filename']\n",
    "                    if filename == img_name[-1]:\n",
    "                        mask = json_data[i]\n",
    "                        \n",
    "        \n",
    "        # area : box의 면적으로써 나중에 IOU구하려고 만든거.\n",
    "        x_min = abs(mask['annot_A']['boxes']['minX'])\n",
    "        x_max = abs(mask['annot_A']['boxes']['maxX'])\n",
    "        y_min = abs(mask['annot_A']['boxes']['minY'])\n",
    "        y_max = abs(mask['annot_A']['boxes']['maxY'])\n",
    "        boxes = [x_min, y_min, x_max, y_max]\n",
    "        boxes = torch.as_tensor(boxes, dtype = torch.float32)\n",
    "        boxes = boxes.unsqueeze(0)\n",
    "        \n",
    "        #area = (boxes[3] - boxes[1]) * (boxes[2] - boxes[0])\n",
    "        \n",
    "        # label\n",
    "        label = self.label_map[mask['faceExp_uploader']]\n",
    "        label = torch.as_tensor(label, dtype=torch.int64)\n",
    "        # return target\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = label.unsqueeze(0)\n",
    "        #target[\"area\"] = area.to(device)\n",
    "        \n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45ebbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbe11658",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize([600, 1000])])\n",
    "#transforms.Resize([600, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa97da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BaegDataset(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "505b6b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.5384, 0.5472, 0.5442,  ..., 0.5492, 0.6115, 0.5982],\n",
       "          [0.5390, 0.5404, 0.5511,  ..., 0.5695, 0.5989, 0.5834],\n",
       "          [0.5490, 0.5391, 0.5509,  ..., 0.6096, 0.6310, 0.6075],\n",
       "          ...,\n",
       "          [0.4623, 0.4694, 0.5045,  ..., 0.5579, 0.5523, 0.5905],\n",
       "          [0.4522, 0.4606, 0.4822,  ..., 0.4971, 0.5386, 0.5433],\n",
       "          [0.4735, 0.4720, 0.4839,  ..., 0.5140, 0.5196, 0.5016]],\n",
       " \n",
       "         [[0.5619, 0.5707, 0.5677,  ..., 0.4103, 0.4589, 0.4296],\n",
       "          [0.5626, 0.5639, 0.5746,  ..., 0.4205, 0.4411, 0.4119],\n",
       "          [0.5725, 0.5626, 0.5744,  ..., 0.4415, 0.4629, 0.4387],\n",
       "          ...,\n",
       "          [0.4623, 0.4694, 0.5045,  ..., 0.5500, 0.5444, 0.5826],\n",
       "          [0.4522, 0.4606, 0.4822,  ..., 0.4854, 0.5268, 0.5315],\n",
       "          [0.4735, 0.4720, 0.4839,  ..., 0.5022, 0.5078, 0.4898]],\n",
       " \n",
       "         [[0.5619, 0.5707, 0.5677,  ..., 0.2881, 0.3164, 0.2939],\n",
       "          [0.5626, 0.5639, 0.5746,  ..., 0.2790, 0.3038, 0.2918],\n",
       "          [0.5725, 0.5626, 0.5744,  ..., 0.3126, 0.3122, 0.2748],\n",
       "          ...,\n",
       "          [0.4230, 0.4302, 0.4653,  ..., 0.5030, 0.4974, 0.5356],\n",
       "          [0.4130, 0.4214, 0.4430,  ..., 0.4566, 0.4980, 0.5027],\n",
       "          [0.4343, 0.4328, 0.4446,  ..., 0.4748, 0.4803, 0.4624]]]),\n",
       " {'boxes': tensor([[1397.7760,  698.9243, 2137.3782, 1684.6287]]),\n",
       "  'labels': tensor([0])})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc6d48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=7, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=28, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine tuning (frozen X)\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "num_classes = 7 # 우린 background 이미지는 없다.\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa2fb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52139"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdc82542",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_datset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7e4ddfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41711\n",
      "10428\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_datset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67717877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84eff244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data_loader = torch.utils.data.DataLoader(test_datset, batch_size = 2, shuffle=False, num_workers=0)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_datset, batch_size = 2, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e46e2ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d3f9881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a0962dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4546186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562232a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    i = 0\n",
    "    epoch_loss = 0\n",
    "    for imgs, labels in train_data_loader:\n",
    "        try:\n",
    "            #img = img.to(device)\n",
    "            #print(type(label))\n",
    "            #label = [{k: v.to(device) for k, v in t.items()} for t in label]\n",
    "            #label = {k : v.to(device) for k, v in label}\n",
    "            i += 1\n",
    "            imgs = list(img.to(device) for img in imgs)\n",
    "            annotations = [{k: v.to(device) for k, v in t.items()} for t in labels]\n",
    "\n",
    "            loss_dict = model(imgs, annotations)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            losses.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += losses\n",
    "            if i == 10000:\n",
    "                print(f'지금 : {i}번째 Loss : {epoch_loss}')\n",
    "        except Exception as e:\n",
    "            logging.error(e, exc_info=True) # log stack trace\n",
    "            continue\n",
    "    print('--------------------------------------------------------------')  \n",
    "    print(f'epoch : {epoch+1}, Loss : {epoch_loss}, time : {time.time() - start}')\n",
    "        \n",
    "        #train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        #ret, predictions = torch.max(output.data, 1)\n",
    "        #correct_counts = predictions.eq(label.data.view_as(predictions))\n",
    "        \n",
    "        #acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "        \n",
    "        #train_acc += acc.item() * inputs.size(0)\n",
    "        \n",
    "        #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4c0c05",
   "metadata": {},
   "source": [
    "시작시간 5시 46분 정도."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf7ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for j, (img, label) in enumerate(test_data_loader):\n",
    "            img = label['image'].to(device)\n",
    "            annotations = label['boxes'].to(device)\n",
    "            label = label['label'].to(device)\n",
    "            inputs = [img, annotations]\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(output, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f91269bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/Emotion_data/Validation/image/c09cf9277d67fdf9d12f1826cd5ba28156abac213264ad154ec993cdb281d01b_남_30_분노_공공시설&종교&의료시설_20201203233638-001-004.jpg\n",
      "성공!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([1170.5842,  643.6230, 1661.2802, 1246.4299]),\n",
       " 'label': 4,\n",
       " 'area': tensor(295794.8750),\n",
       " 'image': tensor([[[0.9107, 0.8853, 0.8940,  ..., 0.1031, 0.1088, 0.1264],\n",
       "          [0.9058, 0.8848, 0.8846,  ..., 0.1318, 0.1287, 0.1110],\n",
       "          [0.8957, 0.9042, 0.9018,  ..., 0.1110, 0.1076, 0.1153],\n",
       "          ...,\n",
       "          [0.8351, 0.8300, 0.8288,  ..., 0.1349, 0.1241, 0.0861],\n",
       "          [0.8341, 0.8276, 0.8319,  ..., 0.1108, 0.1018, 0.0654],\n",
       "          [0.8230, 0.8275, 0.8274,  ..., 0.0902, 0.0840, 0.0834]],\n",
       " \n",
       "         [[0.9381, 0.9127, 0.9175,  ..., 0.1333, 0.1402, 0.1578],\n",
       "          [0.9333, 0.9123, 0.9081,  ..., 0.1619, 0.1601, 0.1407],\n",
       "          [0.9232, 0.9317, 0.9253,  ..., 0.1412, 0.1347, 0.1349],\n",
       "          ...,\n",
       "          [0.8468, 0.8418, 0.8406,  ..., 0.1353, 0.1280, 0.0900],\n",
       "          [0.8459, 0.8394, 0.8436,  ..., 0.1112, 0.1058, 0.0694],\n",
       "          [0.8347, 0.8393, 0.8391,  ..., 0.0906, 0.0879, 0.0873]],\n",
       " \n",
       "         [[0.9107, 0.8853, 0.9018,  ..., 0.1486, 0.1520, 0.1696],\n",
       "          [0.9058, 0.8848, 0.8924,  ..., 0.1772, 0.1719, 0.1531],\n",
       "          [0.8957, 0.9042, 0.9097,  ..., 0.1565, 0.1479, 0.1506],\n",
       "          ...,\n",
       "          [0.8115, 0.8065, 0.8131,  ..., 0.2149, 0.2065, 0.1684],\n",
       "          [0.8106, 0.8041, 0.8162,  ..., 0.1908, 0.1842, 0.1478],\n",
       "          [0.7994, 0.8040, 0.8117,  ..., 0.1702, 0.1586, 0.1657]]]),\n",
       " 'iscrowd': False}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae46fc48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/Emotion_data/Validation/image/46c437c5362ab25921e683613306b7d61b3216c643725852b5fd5a7838e9ffba_여_30_슬픔_상업시설&점포&시장_20210115214547-003-014.jpg\n",
      "성공!!\n",
      "/data/Emotion_data/Validation/image/a6eab57d3daba5dcfef4f1670247a9d398af8b3b8b3d4d53fd11339afb66b156_여_20_중립_상업시설&점포&시장_20210122180603-003-020.jpg\n",
      "성공!!\n",
      "dict_items([('boxes', tensor([[ 809.7222,  813.9601, 1849.5162, 2270.4502],\n",
      "        [ 913.1677,  886.1880, 1758.3567, 2027.0667]])), ('label', tensor([6, 7])), ('area', tensor([1514449.7500,  964258.1250])), ('image', tensor([[[[0.6696, 0.6768, 0.8089,  ..., 0.3132, 0.3073, 0.3170],\n",
      "          [0.6589, 0.6663, 0.8109,  ..., 0.3155, 0.2990, 0.3195],\n",
      "          [0.6613, 0.6827, 0.8235,  ..., 0.3309, 0.3250, 0.3190],\n",
      "          ...,\n",
      "          [0.1289, 0.1795, 0.1191,  ..., 0.7722, 0.7685, 0.7759],\n",
      "          [0.1396, 0.1662, 0.1022,  ..., 0.7695, 0.7793, 0.7618],\n",
      "          [0.1133, 0.1181, 0.0702,  ..., 0.7829, 0.7705, 0.7768]],\n",
      "\n",
      "         [[0.6388, 0.6572, 0.7770,  ..., 0.1754, 0.1679, 0.1611],\n",
      "          [0.6387, 0.6428, 0.7756,  ..., 0.1586, 0.1596, 0.1631],\n",
      "          [0.6378, 0.6490, 0.7882,  ..., 0.1662, 0.1759, 0.1644],\n",
      "          ...,\n",
      "          [0.1191, 0.2057, 0.1269,  ..., 0.4467, 0.4340, 0.4504],\n",
      "          [0.1419, 0.1897, 0.1110,  ..., 0.4440, 0.4420, 0.4363],\n",
      "          [0.1172, 0.1417, 0.0820,  ..., 0.4575, 0.4333, 0.4513]],\n",
      "\n",
      "         [[0.6270, 0.6455, 0.7798,  ..., 0.1677, 0.1529, 0.1611],\n",
      "          [0.6271, 0.6428, 0.7796,  ..., 0.1586, 0.1456, 0.1629],\n",
      "          [0.6378, 0.6524, 0.7921,  ..., 0.1701, 0.1720, 0.1633],\n",
      "          ...,\n",
      "          [0.2034, 0.2515, 0.1857,  ..., 0.3761, 0.3668, 0.3818],\n",
      "          [0.2204, 0.2382, 0.1698,  ..., 0.3734, 0.3754, 0.3725],\n",
      "          [0.1882, 0.1901, 0.1408,  ..., 0.3869, 0.3741, 0.3886]]],\n",
      "\n",
      "\n",
      "        [[[0.3409, 0.3522, 0.3496,  ..., 0.1526, 0.1598, 0.1013],\n",
      "          [0.3160, 0.3247, 0.3556,  ..., 0.1506, 0.1128, 0.0240],\n",
      "          [0.3312, 0.3385, 0.3472,  ..., 0.1377, 0.1239, 0.1193],\n",
      "          ...,\n",
      "          [0.8359, 0.8377, 0.8388,  ..., 0.5568, 0.5936, 0.5642],\n",
      "          [0.8362, 0.8323, 0.8499,  ..., 0.5812, 0.5880, 0.5677],\n",
      "          [0.8374, 0.8425, 0.8466,  ..., 0.5708, 0.5912, 0.5763]],\n",
      "\n",
      "         [[0.3919, 0.4032, 0.4006,  ..., 0.1800, 0.1984, 0.1638],\n",
      "          [0.3670, 0.3757, 0.4066,  ..., 0.1857, 0.1481, 0.0789],\n",
      "          [0.3822, 0.3895, 0.3982,  ..., 0.1752, 0.1592, 0.1625],\n",
      "          ...,\n",
      "          [0.7916, 0.7946, 0.7957,  ..., 0.6744, 0.7112, 0.6818],\n",
      "          [0.7930, 0.7892, 0.8068,  ..., 0.6989, 0.7056, 0.6853],\n",
      "          [0.7943, 0.7993, 0.8035,  ..., 0.6884, 0.7088, 0.6939]],\n",
      "\n",
      "         [[0.4232, 0.4346, 0.4320,  ..., 0.2898, 0.3054, 0.2540],\n",
      "          [0.3984, 0.4071, 0.4380,  ..., 0.2994, 0.2695, 0.1808],\n",
      "          [0.4135, 0.4209, 0.4295,  ..., 0.2957, 0.2807, 0.2723],\n",
      "          ...,\n",
      "          [0.6841, 0.6848, 0.6859,  ..., 0.7686, 0.8132, 0.7759],\n",
      "          [0.6832, 0.6794, 0.6970,  ..., 0.7930, 0.8076, 0.7794],\n",
      "          [0.6766, 0.6895, 0.6937,  ..., 0.7826, 0.8108, 0.7880]]]])), ('iscrowd', tensor([False, False]))])\n",
      "/data/Emotion_data/Validation/image/ab318a800df44f03eaf62085a34ee2ea62da2891870d7fdf041584c97e442576_남_20_슬픔_상업시설&점포&시장_20201204182404-003-066.jpg\n",
      "성공!!\n",
      "/data/Emotion_data/Validation/image/6069ebf03d3e2a1375e31031489bdb81abac8db938e5cdd61e1c89de32d1eb11_남_20_불안_상업시설&점포&시장_20210208154551-003-003.jpg\n",
      "성공!!\n",
      "dict_items([('boxes', tensor([[1248.6410,  598.7239, 1935.5494, 1491.7852],\n",
      "        [1245.3425,  529.7196, 1976.9309, 1414.5388]])), ('label', tensor([6, 5])), ('area', tensor([613451.3125, 647323.4375])), ('image', tensor([[[[0.8108, 0.8234, 0.8163,  ..., 0.8684, 0.8638, 0.8659],\n",
      "          [0.8211, 0.8228, 0.8152,  ..., 0.8509, 0.8683, 0.8626],\n",
      "          [0.8211, 0.8215, 0.8119,  ..., 0.8594, 0.8624, 0.8705],\n",
      "          ...,\n",
      "          [0.3082, 0.3162, 0.3046,  ..., 0.6930, 0.6802, 0.6515],\n",
      "          [0.2908, 0.3178, 0.2922,  ..., 0.7031, 0.6917, 0.6940],\n",
      "          [0.2953, 0.3007, 0.2985,  ..., 0.7217, 0.7228, 0.7125]],\n",
      "\n",
      "         [[0.8226, 0.8351, 0.8281,  ..., 0.8880, 0.8834, 0.8855],\n",
      "          [0.8329, 0.8345, 0.8270,  ..., 0.8705, 0.8879, 0.8822],\n",
      "          [0.8329, 0.8333, 0.8237,  ..., 0.8790, 0.8820, 0.8901],\n",
      "          ...,\n",
      "          [0.3495, 0.3590, 0.3360,  ..., 0.6812, 0.6684, 0.6398],\n",
      "          [0.3397, 0.3606, 0.3143,  ..., 0.6944, 0.6830, 0.6854],\n",
      "          [0.3470, 0.3435, 0.3181,  ..., 0.7060, 0.7071, 0.6968]],\n",
      "\n",
      "         [[0.8422, 0.8547, 0.8477,  ..., 0.9037, 0.8991, 0.9012],\n",
      "          [0.8525, 0.8541, 0.8466,  ..., 0.8862, 0.9036, 0.8979],\n",
      "          [0.8525, 0.8529, 0.8433,  ..., 0.8947, 0.8977, 0.9058],\n",
      "          ...,\n",
      "          [0.2443, 0.2648, 0.2419,  ..., 0.6461, 0.6333, 0.6046],\n",
      "          [0.2320, 0.2665, 0.2271,  ..., 0.6498, 0.6384, 0.6408],\n",
      "          [0.2386, 0.2515, 0.2319,  ..., 0.6589, 0.6600, 0.6498]]],\n",
      "\n",
      "\n",
      "        [[[0.7630, 0.7532, 0.7698,  ..., 0.6752, 0.6695, 0.6606],\n",
      "          [0.7410, 0.7511, 0.7520,  ..., 0.6672, 0.6517, 0.6632],\n",
      "          [0.7369, 0.7534, 0.7437,  ..., 0.6652, 0.6605, 0.6649],\n",
      "          ...,\n",
      "          [0.6829, 0.6803, 0.6969,  ..., 0.6833, 0.6824, 0.6860],\n",
      "          [0.7049, 0.7104, 0.7035,  ..., 0.6814, 0.6817, 0.6744],\n",
      "          [0.7360, 0.7360, 0.7066,  ..., 0.6799, 0.6817, 0.6738]],\n",
      "\n",
      "         [[0.7238, 0.7139, 0.7306,  ..., 0.1811, 0.1754, 0.1665],\n",
      "          [0.7018, 0.7119, 0.7127,  ..., 0.1731, 0.1576, 0.1691],\n",
      "          [0.6977, 0.7142, 0.7045,  ..., 0.1711, 0.1664, 0.1708],\n",
      "          ...,\n",
      "          [0.4751, 0.4725, 0.4891,  ..., 0.5382, 0.5373, 0.5409],\n",
      "          [0.4980, 0.5034, 0.4966,  ..., 0.5363, 0.5366, 0.5293],\n",
      "          [0.5360, 0.5360, 0.5066,  ..., 0.5348, 0.5366, 0.5287]],\n",
      "\n",
      "         [[0.6257, 0.6159, 0.6325,  ..., 0.1340, 0.1283, 0.1194],\n",
      "          [0.6037, 0.6138, 0.6147,  ..., 0.1261, 0.1105, 0.1220],\n",
      "          [0.5997, 0.6161, 0.6064,  ..., 0.1240, 0.1194, 0.1237],\n",
      "          ...,\n",
      "          [0.2711, 0.2686, 0.2852,  ..., 0.3735, 0.3726, 0.3762],\n",
      "          [0.2936, 0.2990, 0.2922,  ..., 0.3716, 0.3719, 0.3646],\n",
      "          [0.3281, 0.3282, 0.2987,  ..., 0.3701, 0.3719, 0.3640]]]])), ('iscrowd', tensor([False, False]))])\n",
      "/data/Emotion_data/Validation/image/d68e712e65fdf47cfd5738a2c05bdb49d3e0e4e8d1381bf845216f82b5f23f91_여_30_상처_숙박 및 거주공간_20210126123049-010-016.jpg\n",
      "성공!!\n",
      "/data/Emotion_data/Validation/image/d2e1753f12f2345a9e23b80e53c93d2177c00df0ef966dc8991c297afda75262_남_20_분노_숙박 및 거주공간_20210119184417-010-010.jpg\n",
      "성공!!\n",
      "dict_items([('boxes', tensor([[1342.2239,  886.3620, 2403.0044, 2306.1255],\n",
      "        [1326.7445,  440.0577, 2070.9346, 1431.3162]])), ('label', tensor([2, 4])), ('area', tensor([1506057.3750,  737684.6875])), ('image', tensor([[[[0.4263, 0.4225, 0.4281,  ..., 0.6555, 0.6562, 0.7025],\n",
      "          [0.4095, 0.4198, 0.4380,  ..., 0.5663, 0.5964, 0.5668],\n",
      "          [0.4163, 0.4368, 0.4332,  ..., 0.5704, 0.6068, 0.5743],\n",
      "          ...,\n",
      "          [0.4966, 0.5090, 0.4789,  ..., 0.6502, 0.6111, 0.8425],\n",
      "          [0.4480, 0.4341, 0.4844,  ..., 0.6415, 0.6117, 0.7521],\n",
      "          [0.4481, 0.5260, 0.5369,  ..., 0.7408, 0.6104, 0.6668]],\n",
      "\n",
      "         [[0.3361, 0.3362, 0.3418,  ..., 0.6006, 0.6052, 0.6711],\n",
      "          [0.3194, 0.3335, 0.3518,  ..., 0.5114, 0.5454, 0.5352],\n",
      "          [0.3261, 0.3506, 0.3470,  ..., 0.5148, 0.5559, 0.5344],\n",
      "          ...,\n",
      "          [0.5554, 0.5678, 0.5377,  ..., 0.6933, 0.6543, 0.8895],\n",
      "          [0.5069, 0.4930, 0.5432,  ..., 0.6847, 0.6549, 0.7991],\n",
      "          [0.5069, 0.5848, 0.5957,  ..., 0.7839, 0.6535, 0.7099]],\n",
      "\n",
      "         [[0.2812, 0.2813, 0.2869,  ..., 0.5653, 0.5699, 0.6280],\n",
      "          [0.2645, 0.2786, 0.2968,  ..., 0.4761, 0.5101, 0.4922],\n",
      "          [0.2712, 0.2957, 0.2921,  ..., 0.4797, 0.5206, 0.4954],\n",
      "          ...,\n",
      "          [0.6456, 0.6580, 0.6279,  ..., 0.7718, 0.7327, 0.9444],\n",
      "          [0.5971, 0.5832, 0.6334,  ..., 0.7631, 0.7333, 0.8618],\n",
      "          [0.5971, 0.6750, 0.6859,  ..., 0.8624, 0.7398, 0.7805]]],\n",
      "\n",
      "\n",
      "        [[[0.7804, 0.7946, 0.7977,  ..., 0.6932, 0.6874, 0.6902],\n",
      "          [0.7807, 0.7951, 0.7935,  ..., 0.6950, 0.7030, 0.6964],\n",
      "          [0.7731, 0.7909, 0.7777,  ..., 0.7176, 0.7019, 0.6945],\n",
      "          ...,\n",
      "          [0.7888, 0.8056, 0.8013,  ..., 0.5911, 0.6034, 0.6147],\n",
      "          [0.7867, 0.8022, 0.7908,  ..., 0.6017, 0.6082, 0.5887],\n",
      "          [0.8054, 0.7912, 0.7801,  ..., 0.5947, 0.5967, 0.5854]],\n",
      "\n",
      "         [[0.3967, 0.4025, 0.4134,  ..., 0.6030, 0.5972, 0.6000],\n",
      "          [0.4081, 0.4093, 0.4165,  ..., 0.5952, 0.6032, 0.5966],\n",
      "          [0.4005, 0.4085, 0.4051,  ..., 0.6156, 0.6000, 0.5925],\n",
      "          ...,\n",
      "          [0.7613, 0.7781, 0.7739,  ..., 0.5272, 0.5362, 0.5481],\n",
      "          [0.7593, 0.7748, 0.7634,  ..., 0.5378, 0.5410, 0.5221],\n",
      "          [0.7780, 0.7638, 0.7527,  ..., 0.5308, 0.5295, 0.5187]],\n",
      "\n",
      "         [[0.3926, 0.3946, 0.4077,  ..., 0.5402, 0.5345, 0.5373],\n",
      "          [0.4003, 0.3983, 0.4083,  ..., 0.5293, 0.5373, 0.5307],\n",
      "          [0.3927, 0.3961, 0.3954,  ..., 0.5489, 0.5333, 0.5259],\n",
      "          ...,\n",
      "          [0.7221, 0.7389, 0.7347,  ..., 0.4871, 0.4970, 0.5089],\n",
      "          [0.7200, 0.7356, 0.7241,  ..., 0.4977, 0.5018, 0.4829],\n",
      "          [0.7387, 0.7246, 0.7135,  ..., 0.4906, 0.4903, 0.4795]]]])), ('iscrowd', tensor([False, False]))])\n",
      "/data/Emotion_data/Validation/image/d5893ed1b754e7adb3dc486225788161a211d05198dee86e91bb4a836421c323_여_20_기쁨_상업시설&점포&시장_20210223160343-003-006.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성공!!\n",
      "/data/Emotion_data/Validation/image/4361bbf013d3f509826169780a2700f0c860cf56ec80189c6a69662b1bb90736_여_50_불안_공공시설&종교&의료시설_20210127225032-001-012.jpg\n",
      "성공!!\n",
      "dict_items([('boxes', tensor([[1287.9668,  213.6841, 2101.7173, 1335.7837],\n",
      "        [1577.9138,  440.8159, 2459.2920, 1592.7188]])), ('label', tensor([1, 5])), ('area', tensor([ 913109.1250, 1015262.0000])), ('image', tensor([[[[0.9350, 0.9387, 0.9298,  ..., 0.7238, 0.7303, 0.7436],\n",
      "          [0.9369, 0.9364, 0.9385,  ..., 0.7122, 0.7421, 0.7388],\n",
      "          [0.9336, 0.9403, 0.9373,  ..., 0.7190, 0.7455, 0.7437],\n",
      "          ...,\n",
      "          [0.6036, 0.6029, 0.6152,  ..., 0.1477, 0.1367, 0.1582],\n",
      "          [0.6099, 0.6017, 0.5999,  ..., 0.1759, 0.2208, 0.1816],\n",
      "          [0.5887, 0.6194, 0.6121,  ..., 0.1280, 0.1907, 0.1483]],\n",
      "\n",
      "         [[0.9193, 0.9230, 0.9141,  ..., 0.6611, 0.6676, 0.6809],\n",
      "          [0.9212, 0.9207, 0.9229,  ..., 0.6495, 0.6793, 0.6760],\n",
      "          [0.9179, 0.9246, 0.9216,  ..., 0.6562, 0.6827, 0.6810],\n",
      "          ...,\n",
      "          [0.6075, 0.6068, 0.6191,  ..., 0.1438, 0.1327, 0.1713],\n",
      "          [0.6138, 0.6057, 0.6039,  ..., 0.1746, 0.2168, 0.1855],\n",
      "          [0.5951, 0.6258, 0.6185,  ..., 0.1269, 0.1849, 0.1519]],\n",
      "\n",
      "         [[0.8761, 0.8799, 0.8788,  ..., 0.5709, 0.5774, 0.5789],\n",
      "          [0.8780, 0.8776, 0.8876,  ..., 0.5593, 0.5891, 0.5741],\n",
      "          [0.8748, 0.8815, 0.8863,  ..., 0.5660, 0.5925, 0.5790],\n",
      "          ...,\n",
      "          [0.5879, 0.5872, 0.5995,  ..., 0.1698, 0.1563, 0.1841],\n",
      "          [0.5942, 0.5860, 0.5842,  ..., 0.1968, 0.2369, 0.2012],\n",
      "          [0.5805, 0.6111, 0.6039,  ..., 0.1451, 0.2045, 0.1678]]],\n",
      "\n",
      "\n",
      "        [[[0.2327, 0.2253, 0.2058,  ..., 0.6387, 0.6460, 0.6406],\n",
      "          [0.2276, 0.2202, 0.2157,  ..., 0.6372, 0.6342, 0.6385],\n",
      "          [0.2194, 0.2202, 0.2197,  ..., 0.6353, 0.6358, 0.6390],\n",
      "          ...,\n",
      "          [0.7410, 0.7407, 0.7334,  ..., 0.9167, 0.9220, 0.9257],\n",
      "          [0.7383, 0.7404, 0.7468,  ..., 0.9192, 0.9220, 0.9225],\n",
      "          [0.7427, 0.7435, 0.7388,  ..., 0.9137, 0.9174, 0.9277]],\n",
      "\n",
      "         [[0.3621, 0.3665, 0.3548,  ..., 0.6270, 0.6342, 0.6288],\n",
      "          [0.3570, 0.3614, 0.3632,  ..., 0.6254, 0.6224, 0.6267],\n",
      "          [0.3606, 0.3613, 0.3609,  ..., 0.6235, 0.6241, 0.6272],\n",
      "          ...,\n",
      "          [0.7057, 0.7054, 0.6981,  ..., 0.3010, 0.3064, 0.3100],\n",
      "          [0.7030, 0.7051, 0.7115,  ..., 0.3035, 0.3063, 0.3068],\n",
      "          [0.7074, 0.7082, 0.7035,  ..., 0.2980, 0.3017, 0.3121]],\n",
      "\n",
      "         [[0.3268, 0.3273, 0.3116,  ..., 0.5995, 0.6067, 0.6014],\n",
      "          [0.3218, 0.3222, 0.3208,  ..., 0.5980, 0.5950, 0.5993],\n",
      "          [0.3214, 0.3221, 0.3216,  ..., 0.5961, 0.5966, 0.5997],\n",
      "          ...,\n",
      "          [0.6783, 0.6779, 0.6707,  ..., 0.2657, 0.2711, 0.2747],\n",
      "          [0.6756, 0.6777, 0.6841,  ..., 0.2682, 0.2710, 0.2715],\n",
      "          [0.6799, 0.6808, 0.6761,  ..., 0.2627, 0.2664, 0.2768]]]])), ('iscrowd', tensor([False, False]))])\n",
      "/data/Emotion_data/Validation/image/c9288346a69a1ddc45aa04965a09d9857485a31743f012b886990bb61b3ed8ef_여_30_기쁨_상업시설&점포&시장_20201203141419-003-017.jpg\n",
      "성공!!\n",
      "/data/Emotion_data/Validation/image/93923c167194ecf117d81e8dd81a64eb641624f70bb29ce316a1ba1c6111e8da_남_30_분노_상업시설&점포&시장_20201208123545-003-003.jpg\n",
      "성공!!\n",
      "dict_items([('boxes', tensor([[1714.5676,  393.6336, 2529.2341, 1522.8051],\n",
      "        [ 674.7552,  193.2251, 1614.2720, 1447.4802]])), ('label', tensor([1, 4])), ('area', tensor([ 919898.1250, 1178393.7500])), ('image', tensor([[[[0.5367, 0.5647, 0.5438,  ..., 0.0892, 0.1691, 0.0774],\n",
      "          [0.5264, 0.5508, 0.5149,  ..., 0.0984, 0.1756, 0.0577],\n",
      "          [0.4879, 0.5066, 0.5242,  ..., 0.1267, 0.0619, 0.0563],\n",
      "          ...,\n",
      "          [0.0891, 0.0923, 0.1048,  ..., 0.2779, 0.3921, 0.3962],\n",
      "          [0.0946, 0.0885, 0.1084,  ..., 0.3161, 0.3925, 0.0838],\n",
      "          [0.1065, 0.0883, 0.1034,  ..., 0.3203, 0.2798, 0.1227]],\n",
      "\n",
      "         [[0.5445, 0.5725, 0.5595,  ..., 0.0971, 0.1769, 0.0852],\n",
      "          [0.5343, 0.5587, 0.5306,  ..., 0.1063, 0.1835, 0.0655],\n",
      "          [0.4957, 0.5144, 0.5399,  ..., 0.1346, 0.0697, 0.0641],\n",
      "          ...,\n",
      "          [0.0970, 0.1119, 0.1048,  ..., 0.3982, 0.5115, 0.5009],\n",
      "          [0.1025, 0.1081, 0.1084,  ..., 0.4573, 0.5100, 0.1740],\n",
      "          [0.1143, 0.1080, 0.1034,  ..., 0.4615, 0.3780, 0.2184]],\n",
      "\n",
      "         [[0.5406, 0.5686, 0.5555,  ..., 0.0931, 0.1730, 0.0813],\n",
      "          [0.5304, 0.5548, 0.5266,  ..., 0.1023, 0.1796, 0.0616],\n",
      "          [0.4918, 0.5105, 0.5360,  ..., 0.1306, 0.0658, 0.0602],\n",
      "          ...,\n",
      "          [0.0852, 0.0963, 0.1048,  ..., 0.9572, 0.9967, 0.9627],\n",
      "          [0.0907, 0.0924, 0.1084,  ..., 0.9895, 0.9751, 0.7876],\n",
      "          [0.1025, 0.0923, 0.1034,  ..., 0.9927, 0.9466, 0.8383]]],\n",
      "\n",
      "\n",
      "        [[[0.6821, 0.6518, 0.6302,  ..., 0.9973, 0.9882, 0.9882],\n",
      "          [0.6852, 0.6180, 0.6225,  ..., 0.9919, 0.9882, 0.9882],\n",
      "          [0.6915, 0.6392, 0.6090,  ..., 0.9910, 0.9882, 0.9882],\n",
      "          ...,\n",
      "          [0.4327, 0.4235, 0.4393,  ..., 0.0970, 0.0944, 0.1094],\n",
      "          [0.4490, 0.4265, 0.4380,  ..., 0.0995, 0.0969, 0.1125],\n",
      "          [0.4343, 0.4526, 0.4370,  ..., 0.0945, 0.0894, 0.1045]],\n",
      "\n",
      "         [[0.4115, 0.3851, 0.3580,  ..., 0.9933, 0.9922, 0.9922],\n",
      "          [0.4068, 0.3630, 0.3558,  ..., 0.9880, 0.9922, 0.9922],\n",
      "          [0.4091, 0.3842, 0.3502,  ..., 0.9871, 0.9922, 0.9922],\n",
      "          ...,\n",
      "          [0.4131, 0.4039, 0.4080,  ..., 0.1323, 0.1297, 0.1447],\n",
      "          [0.4294, 0.4069, 0.4067,  ..., 0.1348, 0.1322, 0.1478],\n",
      "          [0.4147, 0.4330, 0.4057,  ..., 0.1298, 0.1247, 0.1398]],\n",
      "\n",
      "         [[0.2586, 0.2479, 0.2235,  ..., 0.9149, 0.9216, 0.9216],\n",
      "          [0.2565, 0.2142, 0.2186,  ..., 0.9096, 0.9216, 0.9216],\n",
      "          [0.2601, 0.2355, 0.2130,  ..., 0.9086, 0.9216, 0.9216],\n",
      "          ...,\n",
      "          [0.4014, 0.3922, 0.4001,  ..., 0.1284, 0.1258, 0.1408],\n",
      "          [0.4176, 0.3951, 0.3988,  ..., 0.1309, 0.1283, 0.1439],\n",
      "          [0.4030, 0.4213, 0.3978,  ..., 0.1259, 0.1208, 0.1359]]]])), ('iscrowd', tensor([False, False]))])\n",
      "/data/Emotion_data/Validation/image/8319fbabcccb7f9595da623934586295323a06d7d84f393a2b3db03b85994d42_남_20_중립_실외 자연환경_20201204000225-007-003.jpg\n",
      "성공!!\n",
      "/data/Emotion_data/Validation/image/67e813fcbdf35349f27bc06d7dc4ab292f8c1e0b164b52327a835d214e874943_여_50_분노_상업시설&점포&시장_20210125193723-003-003.jpg\n",
      "성공!!\n",
      "dict_items([('boxes', tensor([[1344.8666,  628.3788, 2193.2305, 1632.2064],\n",
      "        [2145.2339, 1098.2999, 2978.9912, 2203.0918]])), ('label', tensor([7, 4])), ('area', tensor([851611.1250, 921128.3125])), ('image', tensor([[[[0.9811, 0.9492, 0.7722,  ..., 0.6563, 0.6680, 0.8328],\n",
      "          [0.9847, 0.8804, 0.7127,  ..., 0.6292, 0.7279, 0.8840],\n",
      "          [0.9809, 0.8333, 0.7526,  ..., 0.6088, 0.7757, 0.8823],\n",
      "          ...,\n",
      "          [0.6718, 0.6075, 0.6200,  ..., 0.1898, 0.1983, 0.1816],\n",
      "          [0.6195, 0.5688, 0.5825,  ..., 0.1845, 0.1654, 0.1977],\n",
      "          [0.5512, 0.5855, 0.5892,  ..., 0.2018, 0.1940, 0.1966]],\n",
      "\n",
      "         [[0.9768, 0.9390, 0.7406,  ..., 0.6294, 0.7812, 0.9027],\n",
      "          [0.9721, 0.8929, 0.6889,  ..., 0.6130, 0.8314, 0.9375],\n",
      "          [0.9646, 0.8214, 0.7317,  ..., 0.6066, 0.8517, 0.9334],\n",
      "          ...,\n",
      "          [0.6484, 0.5878, 0.6003,  ..., 0.2134, 0.2219, 0.2051],\n",
      "          [0.5921, 0.5458, 0.5629,  ..., 0.2081, 0.1890, 0.2213],\n",
      "          [0.5238, 0.5619, 0.5696,  ..., 0.2254, 0.2175, 0.2201]],\n",
      "\n",
      "         [[0.8343, 0.7410, 0.5209,  ..., 0.3115, 0.6646, 0.9473],\n",
      "          [0.8068, 0.6999, 0.4826,  ..., 0.2845, 0.7225, 0.9464],\n",
      "          [0.7753, 0.6539, 0.5349,  ..., 0.2867, 0.7604, 0.9059],\n",
      "          ...,\n",
      "          [0.5621, 0.5133, 0.5226,  ..., 0.3075, 0.3081, 0.2914],\n",
      "          [0.5183, 0.4814, 0.4884,  ..., 0.3007, 0.2767, 0.3090],\n",
      "          [0.4532, 0.4992, 0.4917,  ..., 0.3116, 0.3116, 0.3142]]],\n",
      "\n",
      "\n",
      "        [[[0.5437, 0.5121, 0.4975,  ..., 0.5643, 0.5702, 0.5689],\n",
      "          [0.5462, 0.5145, 0.5130,  ..., 0.5821, 0.5653, 0.5639],\n",
      "          [0.5458, 0.5147, 0.5059,  ..., 0.5562, 0.5538, 0.5655],\n",
      "          ...,\n",
      "          [0.3288, 0.3250, 0.3203,  ..., 0.7341, 0.6696, 0.6756],\n",
      "          [0.3381, 0.3191, 0.3373,  ..., 0.7132, 0.6742, 0.6662],\n",
      "          [0.3339, 0.3175, 0.3386,  ..., 0.7066, 0.6613, 0.6557]],\n",
      "\n",
      "         [[0.6946, 0.6572, 0.6544,  ..., 0.5368, 0.5427, 0.5414],\n",
      "          [0.6927, 0.6519, 0.6542,  ..., 0.5546, 0.5378, 0.5364],\n",
      "          [0.6896, 0.6559, 0.6471,  ..., 0.5288, 0.5263, 0.5381],\n",
      "          ...,\n",
      "          [0.3013, 0.2975, 0.2929,  ..., 0.6165, 0.5520, 0.5580],\n",
      "          [0.3122, 0.2932, 0.3114,  ..., 0.5956, 0.5566, 0.5486],\n",
      "          [0.3104, 0.2939, 0.3151,  ..., 0.5890, 0.5436, 0.5381]],\n",
      "\n",
      "         [[0.7492, 0.6925, 0.6936,  ..., 0.2702, 0.2761, 0.2748],\n",
      "          [0.7547, 0.6961, 0.7013,  ..., 0.2879, 0.2712, 0.2697],\n",
      "          [0.7496, 0.7056, 0.6968,  ..., 0.2621, 0.2596, 0.2714],\n",
      "          ...,\n",
      "          [0.2621, 0.2583, 0.2536,  ..., 0.5145, 0.4422, 0.4524],\n",
      "          [0.2699, 0.2509, 0.2691,  ..., 0.4936, 0.4468, 0.4430],\n",
      "          [0.2633, 0.2469, 0.2680,  ..., 0.4870, 0.4338, 0.4325]]]])), ('iscrowd', tensor([False, False]))])\n",
      "/data/Emotion_data/Validation/image/2c6cedc4c9ea7c0614e02d0fee8b32f15872dc7c096d6e1ff2c28728d8c6dfde_남_30_분노_행사&사무공간_20201205171600-006-006.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성공!!\n",
      "/data/Emotion_data/Validation/image/6510399d39bee68c097512c98d7fe446a7e100309970091d5c84efcffe6a88ce_여_20_중립_도심 환경_20210208145437-008-005.jpg\n",
      "성공!!\n",
      "dict_items([('boxes', tensor([[1656.3026,  820.2828, 2584.6660, 2132.4058],\n",
      "        [1703.0969,  570.7196, 2483.3887, 1632.6982]])), ('label', tensor([4, 7])), ('area', tensor([1218126.8750,  828653.1875])), ('image', tensor([[[[0.8286, 0.8402, 0.8291,  ..., 0.8059, 0.8414, 0.8181],\n",
      "          [0.8351, 0.8258, 0.8600,  ..., 0.8323, 0.8415, 0.8467],\n",
      "          [0.8423, 0.8368, 0.8380,  ..., 0.8296, 0.8388, 0.8501],\n",
      "          ...,\n",
      "          [0.8342, 0.8354, 0.8065,  ..., 0.8043, 0.8343, 0.8144],\n",
      "          [0.8137, 0.8055, 0.8162,  ..., 0.8253, 0.8374, 0.8074],\n",
      "          [0.8246, 0.8238, 0.8181,  ..., 0.7868, 0.8504, 0.8346]],\n",
      "\n",
      "         [[0.8168, 0.8285, 0.8174,  ..., 0.7902, 0.8139, 0.8024],\n",
      "          [0.8234, 0.8140, 0.8482,  ..., 0.8166, 0.8140, 0.8310],\n",
      "          [0.8306, 0.8250, 0.8262,  ..., 0.8139, 0.8114, 0.8344],\n",
      "          ...,\n",
      "          [0.8062, 0.8075, 0.7791,  ..., 0.7690, 0.7990, 0.7791],\n",
      "          [0.7863, 0.7781, 0.7888,  ..., 0.7900, 0.8021, 0.7721],\n",
      "          [0.8089, 0.8081, 0.7906,  ..., 0.7515, 0.8151, 0.7993]],\n",
      "\n",
      "         [[0.7972, 0.8089, 0.7978,  ..., 0.7549, 0.7826, 0.7593],\n",
      "          [0.8037, 0.7944, 0.8286,  ..., 0.7813, 0.7826, 0.7879],\n",
      "          [0.8109, 0.8054, 0.8066,  ..., 0.7786, 0.7800, 0.7913],\n",
      "          ...,\n",
      "          [0.7825, 0.7837, 0.7555,  ..., 0.7415, 0.7715, 0.7517],\n",
      "          [0.7627, 0.7546, 0.7653,  ..., 0.7625, 0.7746, 0.7446],\n",
      "          [0.7619, 0.7611, 0.7671,  ..., 0.7240, 0.7877, 0.7719]]],\n",
      "\n",
      "\n",
      "        [[[0.4078, 0.3445, 0.3828,  ..., 0.7874, 0.6944, 0.7565],\n",
      "          [0.4012, 0.3405, 0.3708,  ..., 0.8201, 0.6957, 0.7268],\n",
      "          [0.4015, 0.3511, 0.3575,  ..., 0.8435, 0.6977, 0.8011],\n",
      "          ...,\n",
      "          [0.5159, 0.4937, 0.5079,  ..., 0.3059, 0.2973, 0.2755],\n",
      "          [0.4977, 0.5100, 0.5730,  ..., 0.2685, 0.2944, 0.2854],\n",
      "          [0.4892, 0.4955, 0.5119,  ..., 0.2642, 0.2835, 0.2780]],\n",
      "\n",
      "         [[0.5297, 0.4739, 0.5048,  ..., 0.8658, 0.7689, 0.8296],\n",
      "          [0.5242, 0.4777, 0.4935,  ..., 0.8978, 0.7648, 0.7902],\n",
      "          [0.5270, 0.4884, 0.4830,  ..., 0.9177, 0.7668, 0.8631],\n",
      "          ...,\n",
      "          [0.5394, 0.5172, 0.5315,  ..., 0.3333, 0.3193, 0.2912],\n",
      "          [0.5095, 0.5217, 0.5847,  ..., 0.2959, 0.3164, 0.3011],\n",
      "          [0.4963, 0.5026, 0.5190,  ..., 0.2894, 0.3034, 0.2937]],\n",
      "\n",
      "         [[0.5101, 0.4464, 0.4852,  ..., 0.8583, 0.7574, 0.8125],\n",
      "          [0.5046, 0.4503, 0.4739,  ..., 0.8904, 0.7552, 0.7763],\n",
      "          [0.5074, 0.4609, 0.4634,  ..., 0.9106, 0.7571, 0.8499],\n",
      "          ...,\n",
      "          [0.5002, 0.4780, 0.4922,  ..., 0.3647, 0.3603, 0.3344],\n",
      "          [0.4742, 0.4865, 0.5494,  ..., 0.3335, 0.3575, 0.3468],\n",
      "          [0.4633, 0.4696, 0.4860,  ..., 0.3332, 0.3505, 0.3407]]]])), ('iscrowd', tensor([False, False]))])\n",
      "/data/Emotion_data/Validation/image/9704ac9d65181ac1c8f42cacfac9e6aa2a80f6f32d0b524cbb7b4ec8f138c68f_여_50_중립_교통&이동수단(엘리베이터 포함)_20201207180956-002-007.jpg\n",
      "성공!!\n",
      "/data/Emotion_data/Validation/image/b1de988d9e05002f78db80e481b5621bc66785986f272d3f02522e1d318b9ce0_여_20_슬픔_스포츠 관람 및 레저시설_20201207191856-004-006.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52431/3656590302.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch-1.9.1-py3.8-linux-x86_64.egg/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch-1.9.1-py3.8-linux-x86_64.egg/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch-1.9.1-py3.8-linux-x86_64.egg/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch-1.9.1-py3.8-linux-x86_64.egg/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch-1.9.1-py3.8-linux-x86_64.egg/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_52431/1621628838.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mjson_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mjson_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for boxes, label in enumerate(train_data_loader, 0):\n",
    "    print(label.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "856eea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {1:2, 3:4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "274dcc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(1, 2), (3, 4)])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c96f5dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cecda8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.as_tensor(labels, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3f82128a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff3032b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
