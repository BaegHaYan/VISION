{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fd8ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab80a6",
   "metadata": {},
   "source": [
    "'''\n",
    "{'filename': '5f656a0f627a3ef96dec882437e3e7ada1c7a877201cf54dcd7a2c4508588ff3_여_30_기쁨_공공시설&종교&의료시설_20201204105732-001-007.jpg',\n",
    " 'gender': '여',\n",
    " 'age': 30,\n",
    " 'isProf': '전문인',\n",
    " 'faceExp_uploader': '기쁨',\n",
    " 'bg_uploader': '공공시설/종교/의료시설',\n",
    " 'annot_A': {'boxes': {'maxX': 1912.2253,\n",
    "   'maxY': 1581.6027,\n",
    "   'minX': 1187.4949,\n",
    "   'minY': 579.22235},\n",
    "  'faceExp': '기쁨',\n",
    "  'bg': '공공시설/종교/의료'},\n",
    " 'annot_B': {'boxes': {'maxX': 1912.348108621648,\n",
    "   'maxY': 1572.1522585800617,\n",
    "   'minX': 1206.363701502596,\n",
    "   'minY': 579.1777983055337},\n",
    "  'faceExp': '기쁨',\n",
    "  'bg': '공공시설/종교/의료'},\n",
    " 'annot_C': {'boxes': {'maxX': 1890.909447114109,\n",
    "   'maxY': 1567.448627450284,\n",
    "   'minX': 1183.8414475546967,\n",
    "   'minY': 596.9434661684523},\n",
    "  'faceExp': '기쁨',\n",
    "  'bg': '공공시설/종교/의료'}}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf704aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaegDataset(Dataset):\n",
    "    def __init__(self ,mode = 'train', transform = None):\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        # image dataset\n",
    "        # image dataset 병합\n",
    "        if self.mode == 'train':\n",
    "            upset_list = glob(\"/data/Emotion_data/Training/upset/*\")\n",
    "            pleasure_list = glob(\"/data/Emotion_data/Training/pleasure/*\")\n",
    "            hurt_list = glob(\"/data/Emotion_data/Training/hurt/*\")\n",
    "            anger_list = glob(\"/data/Emotion_data/Training/anger/*\")\n",
    "            unrest_list = glob(\"/data/Emotion_data/Training/unrest/*\")\n",
    "            sad_list = glob(\"/data/Emotion_data/Training/sad/*\")\n",
    "            neutrality_list = glob(\"/data/Emotion_data/Training/neutrality/*\")\n",
    "            self.data_list = upset_list + pleasure_list + anger_list + unrest_list + sad_list + neutrality_list\n",
    "        \n",
    "            # json dataset\n",
    "            self.label_list = glob(\"/data/Emotion_data/Training/label/*\")\n",
    "        elif self.mode == 'test':\n",
    "            self.data_list = glob('/data/Emotion_data/Validation/image/*')\n",
    "            self.label_list = glob(\"/data/Emotion_data/Validation/label/*\")\n",
    "            \n",
    "        # label map\n",
    "        self.label_map = {\n",
    "            '기쁨' : 1,\n",
    "            '상처' : 2,\n",
    "            '당황' : 3,\n",
    "            '분노' : 4,\n",
    "            '불안' : 5,\n",
    "            '슬픔' : 6,\n",
    "            '중립' : 7\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # load images and mask\n",
    "        img_path = self.data_list[idx]\n",
    "        print(self.data_list[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform  is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        # 1. filename만 따로 빼서 for문 돌려서 json_list에 있는 것과 비교\n",
    "        img_name = img_path.split('/')\n",
    "        mask = {}\n",
    "        for json_list in self.label_list:\n",
    "            with open(json_list, 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "                for i in range(0, len(json_data)):\n",
    "                    filename = json_data[i]['filename']\n",
    "                    if filename == img_name[-1]:\n",
    "                        print(\"성공!!\")\n",
    "                        mask = json_data[i]\n",
    "                        \n",
    "        \n",
    "        # area : box의 면적으로써 나중에 IOU구하려고 만든거.\n",
    "        x_min = mask['annot_A']['boxes']['minX']\n",
    "        x_max = mask['annot_A']['boxes']['maxX']\n",
    "        y_min = mask['annot_A']['boxes']['minY']\n",
    "        y_max = mask['annot_A']['boxes']['maxY']\n",
    "        boxes = [x_min, y_min, x_max, y_max]\n",
    "        boxes = torch.as_tensor(boxes, dtype = torch.float32)\n",
    "        \n",
    "        area = (boxes[3] - boxes[1]) * (boxes[2] - boxes[0])\n",
    "        \n",
    "        # label\n",
    "        label = self.label_map[mask['faceExp_uploader']]\n",
    "        \n",
    "        # return target\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"label\"] = label\n",
    "        target[\"area\"] = area\n",
    "        target['image'] = img\n",
    "        target['iscrowd'] = False\n",
    "\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbe11658",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa97da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BaegDataset(\"test\", transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "505b6b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/Emotion_data/Validation/image/006b56dc2f8cda2361e1b01b2496d6f352dd5b1790f0a9b0bfcbe540b292247d_여_20_기쁨_공공시설&종교&의료시설_20210130213913-001-009.jpg\n",
      "성공!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([1419.8829,  361.9550, 2229.0037, 1447.1696]),\n",
       " 'label': 1,\n",
       " 'area': tensor(878069.6250),\n",
       " 'image': tensor([[[0.1412, 0.1804, 0.1843,  ..., 0.8118, 0.7922, 0.8157],\n",
       "          [0.1451, 0.1608, 0.1686,  ..., 0.8118, 0.8118, 0.8157],\n",
       "          [0.1961, 0.1725, 0.1686,  ..., 0.8078, 0.8275, 0.8118],\n",
       "          ...,\n",
       "          [0.0902, 0.0902, 0.0902,  ..., 0.7137, 0.7137, 0.7137],\n",
       "          [0.0980, 0.0980, 0.0980,  ..., 0.7137, 0.7098, 0.7098],\n",
       "          [0.1020, 0.1020, 0.0980,  ..., 0.7137, 0.7137, 0.7137]],\n",
       " \n",
       "         [[0.1333, 0.1725, 0.1765,  ..., 0.8863, 0.8667, 0.8902],\n",
       "          [0.1373, 0.1529, 0.1608,  ..., 0.8863, 0.8863, 0.8902],\n",
       "          [0.1882, 0.1647, 0.1608,  ..., 0.8824, 0.9020, 0.8863],\n",
       "          ...,\n",
       "          [0.0902, 0.0902, 0.0902,  ..., 0.7176, 0.7176, 0.7176],\n",
       "          [0.0980, 0.0980, 0.0980,  ..., 0.7176, 0.7216, 0.7216],\n",
       "          [0.1020, 0.1020, 0.0980,  ..., 0.7255, 0.7255, 0.7255]],\n",
       " \n",
       "         [[0.1373, 0.1765, 0.1804,  ..., 0.9098, 0.8902, 0.9137],\n",
       "          [0.1412, 0.1569, 0.1647,  ..., 0.9098, 0.9098, 0.9137],\n",
       "          [0.1922, 0.1686, 0.1647,  ..., 0.9059, 0.9255, 0.9098],\n",
       "          ...,\n",
       "          [0.0902, 0.0902, 0.0902,  ..., 0.6863, 0.6863, 0.6863],\n",
       "          [0.0980, 0.0980, 0.1059,  ..., 0.6863, 0.6863, 0.6863],\n",
       "          [0.1020, 0.1098, 0.1059,  ..., 0.6902, 0.6902, 0.6902]]]),\n",
       " 'iscrowd': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc6d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning (frozen X)\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "\n",
    "num_classes = 7 # 우린 background 이미지는 없다.\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
